# GAIA Predictive Maintenance Platform (PDM-PILOT)

> **Enterprise-grade Industrial IoT platform for real-time predictive maintenance, anomaly detection, and AI-powered maintenance recommendations.**
>
> **Standards:** ISO 55001 (Asset Management) | ISO 13372/13374 (Condition Monitoring & Diagnostics)

---

## ğŸ“‹ Project Overview

**GAIA** is a full-stack predictive maintenance system designed for manufacturing environments. It ingests real-time sensor telemetry (vibration, temperature, torque, rotational speed), runs ML inference to predict equipment failures, and provides actionable maintenance recommendations through a modern React dashboard.

### Core Capabilities
- **Real-time Telemetry Streaming** via Redis Pub/Sub + WebSocket
- **ML Failure Prediction** using XGBoost/Scikit-learn models
- **Bearing Fault Detection** (BPFO/BPFI frequency analysis)
- **Remaining Useful Life (RUL)** estimation
- **AI Maintenance Recommendations** (OpenAI/Azure OpenAI integration)
- **Enterprise Features**: Alarms, Work Orders, MTBF/MTTR, Shift Schedules
- **Anomaly Discovery Engine** (Isolation Forest, Temporal Autoencoder)

---

## ğŸ—ï¸ Architecture

### 6-Stage Pipeline (ISO Golden Standard)

The platform follows a **compartmentalized 6-stage pipeline**. Each stage is isolated; no cross-stage bleed.

| Stage | Name | Responsibility |
|-------|------|-----------------|
| 1 | **INGEST** | OPC-UA / File Watcher â†’ `raw_sensor_data` |
| 2 | **CLEANSE** | Outlier removal, feature extraction â†’ `clean_features` |
| 3 | **CONTEXT** | Digital Twin mapping â†’ `contextualized_data` |
| 4 | **PERSIST** | TimescaleDB batch writer |
| 5 | **INFER** | XGBoost RUL prediction â†’ `inference_results` |
| 6 | **ORCH** | API Gateway + Frontend |

See **[docs/ARCHITECTURE.md](./docs/ARCHITECTURE.md)** for full stage mapping and compartment boundaries.

**Standards alignment:**
- [docs/ISO_55001_MAPPING.md](./docs/ISO_55001_MAPPING.md) â€” Asset management (ISO 55001)
- [docs/ISO_13372_MAPPING.md](./docs/ISO_13372_MAPPING.md) â€” Condition monitoring (ISO 13372/13374)

---

### System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FRONTEND (React + Vite)                       â”‚
â”‚   FleetTreemap â”‚ MachineDetail â”‚ WorkOrderPanel â”‚ AnomalyDiscovery   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API LAYER (FastAPI)                             â”‚
â”‚  api_server.py    â”‚   enterprise_api.py   â”‚   anomaly_discovery/api  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  /health          â”‚   /api/enterprise/*   â”‚   /api/discovery/*       â”‚
â”‚  /api/machines    â”‚   /api/enterprise/token (Auth)                   â”‚
â”‚  /api/features    â”‚   /api/enterprise/alarms                         â”‚
â”‚  /api/recommendations â”‚ /api/enterprise/work-orders                  â”‚
â”‚  /ws/stream       â”‚   /api/enterprise/reliability                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Redis      â”‚         â”‚  PostgreSQL/    â”‚         â”‚  ML Models      â”‚
â”‚  (Pub/Sub)    â”‚         â”‚  TimescaleDB    â”‚         â”‚  (gaia_model.pkl)â”‚
â”‚ sensor_stream â”‚         â”‚  cwru_features  â”‚         â”‚  (rul_model.json)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  pdm_alarms     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                 â”‚  work_orders    â”‚
        â”‚                 â”‚  failure_events â”‚
        â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA INGESTION LAYER                               â”‚
â”‚  stream_publisher.py â”‚ stream_consumer.py â”‚ mock_fleet_streamer.py   â”‚
â”‚  opc_client_adapter.py (OPC-UA) â”‚ high_fidelity_simulator.py         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Directory Structure

```
PDM-PILOT/
â”œâ”€â”€ api_server.py           # Main FastAPI application (REST + WebSocket)
â”œâ”€â”€ enterprise_api.py       # Enterprise endpoints (alarms, work orders, auth)
â”œâ”€â”€ config.py               # Centralized settings (Pydantic Settings)
â”œâ”€â”€ database.py             # Async SQLAlchemy engine & session management
â”œâ”€â”€ dependencies.py         # FastAPI dependencies (auth, DB sessions)
â”œâ”€â”€ logger.py               # Structured logging (structlog + JSON)
â”‚
â”œâ”€â”€ core/                   # Core infrastructure
â”‚   â”œâ”€â”€ exceptions.py       # Custom exception hierarchy
â”‚   â”œâ”€â”€ logger.py           # Request ID middleware & logging
â”‚   â””â”€â”€ monitoring.py       # Prometheus metrics (stub)
â”‚
â”œâ”€â”€ schemas/                # Pydantic models (request/response validation)
â”‚   â”œâ”€â”€ __init__.py         # Validated schemas (AlarmCreate, WorkOrder, etc.)
â”‚   â”œâ”€â”€ security.py         # Token, User, UserRole schemas
â”‚   â”œâ”€â”€ response.py         # APIResponse wrapper, ORJSONResponse
â”‚   â”œâ”€â”€ enterprise.py       # WorkOrder schema
â”‚   â”œâ”€â”€ machine.py          # Machine status schemas
â”‚   â””â”€â”€ ml.py               # ML prediction schemas
â”‚
â”œâ”€â”€ services/               # Business logic layer (async)
â”‚   â”œâ”€â”€ base.py             # BaseService with DB session
â”‚   â”œâ”€â”€ auth_service.py     # JWT creation, password hashing (bcrypt)
â”‚   â”œâ”€â”€ machine_service.py  # Machine status & RUL calculations
â”‚   â”œâ”€â”€ alarm_service.py    # Alarm CRUD & threshold checks
â”‚   â””â”€â”€ telemetry_service.py # Telemetry ingestion & batch processing
â”‚
â”œâ”€â”€ middleware/             # FastAPI middleware
â”‚   â”œâ”€â”€ security_headers.py # CSP, HSTS, X-Frame-Options (Helmet.js equiv)
â”‚   â””â”€â”€ audit_logger.py     # Audit logging for sensitive operations
â”‚
â”œâ”€â”€ anomaly_discovery/      # Anomaly detection subsystem
â”‚   â”œâ”€â”€ api.py              # Discovery API routes
â”‚   â”œâ”€â”€ discovery_engine.py # Main orchestrator
â”‚   â”œâ”€â”€ detectors/          # IsolationForest, TemporalAutoencoder
â”‚   â””â”€â”€ analyzers/          # Correlation analysis
â”‚
â”œâ”€â”€ frontend/               # React + Vite frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx         # Main app with routing
â”‚   â”‚   â”œâ”€â”€ components/     # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ FleetTreemap.jsx      # Fleet health overview
â”‚   â”‚   â”‚   â”œâ”€â”€ MachineDetail.jsx     # Individual machine view
â”‚   â”‚   â”‚   â”œâ”€â”€ WorkOrderPanel.jsx    # Maintenance work orders
â”‚   â”‚   â”‚   â”œâ”€â”€ ActiveAlarmFeed.jsx   # Real-time alarms
â”‚   â”‚   â”‚   â”œâ”€â”€ RULCard.jsx           # Remaining Useful Life
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.js       # WebSocket hook for /ws/stream
â”‚   â”‚   â””â”€â”€ theme.js        # MUI theme configuration
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ asset_management/      # ISO 55001 alignment
â”‚   â””â”€â”€ README.md          # Clause â†’ implementation mapping
â”œâ”€â”€ condition_monitoring/  # ISO 13372/13374 alignment
â”‚   â””â”€â”€ README.md          # Part â†’ implementation mapping
â”œâ”€â”€ pipeline/              # Stages 1â€“5 (compartmentalized)
â”‚   â”œâ”€â”€ ingestion/         # Stage 1
â”‚   â”œâ”€â”€ cleansing/         # Stage 2
â”‚   â”œâ”€â”€ contextualization/ # Stage 3
â”‚   â”œâ”€â”€ persistence/       # Stage 4
â”‚   â””â”€â”€ inference/         # Stage 5
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md    # 6-stage pipeline
â”‚   â”œâ”€â”€ ISO_55001_MAPPING.md
â”‚   â””â”€â”€ ISO_13372_MAPPING.md
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ local_backup.py     # Docker pg_dump backup with rotation
â”‚   â”œâ”€â”€ secure_backup.py    # Encrypted S3 backup
â”‚   â”œâ”€â”€ migrate_enterprise_features.py  # DB migrations
â”‚   â””â”€â”€ check_audit_logs.py # Verify audit logging
â”‚
â”œâ”€â”€ dashboards/             # Legacy/deprecated dashboards
â”‚   â””â”€â”€ _DEPRECATED_dashboard.py
â”‚
â”œâ”€â”€ demos/                  # Demo & simulation scripts
â”‚   â”œâ”€â”€ agent_demo.py       # AI agent demonstration
â”‚   â””â”€â”€ start_demo.py       # Demo launcher
â”‚
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ test_schema_validation.py
â”‚   â””â”€â”€ test_bad_data.py
â”‚
â”œâ”€â”€ docker-compose.yml      # Full stack: API, TimescaleDB, Redis
â”œâ”€â”€ Dockerfile              # API container build
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ .env.example            # Environment template
```

---

## ğŸ”§ Technology Stack

### Backend
| Component | Technology |
|-----------|------------|
| **Framework** | FastAPI (async) |
| **Database** | PostgreSQL + TimescaleDB (time-series) |
| **ORM** | SQLAlchemy 2.0 (async) + Alembic migrations |
| **Cache/Pub-Sub** | Redis (aioredis) |
| **Auth** | JWT (python-jose) + bcrypt (passlib) |
| **Rate Limiting** | slowapi (100/min global, 5/min on login) |
| **Logging** | structlog (JSON format) |
| **ML** | scikit-learn, XGBoost, joblib |

### Frontend
| Component | Technology |
|-----------|------------|
| **Framework** | React 18 + Vite |
| **UI Library** | Material-UI (MUI) |
| **Charts** | Recharts |
| **State** | React hooks + WebSocket |
| **HTTP** | fetch API |

### Infrastructure
| Component | Technology |
|-----------|------------|
| **Containers** | Docker + docker-compose |
| **Time-Series DB** | TimescaleDB (PostgreSQL extension) |
| **Message Queue** | Redis Pub/Sub |
| **OPC-UA** | asyncua (industrial protocol) |

---

## ğŸ” Security Features

1. **Authentication**: JWT tokens with 30-min expiry
2. **Rate Limiting**: 100 req/min global, 5 req/min on `/token`
3. **Security Headers**: CSP, HSTS, X-Frame-Options, X-Content-Type-Options
4. **Input Validation**: Pydantic schemas with strict rules
5. **Audit Logging**: Sensitive operations logged with user context
6. **Global Exception Handler**: No stack traces leaked to clients
7. **RBAC**: `get_current_user` and `get_current_admin_user` dependencies

---

## Federated Deployment

When to use **federated** vs **centralized** training:

- **Federated learning (Flower):** Use when data must stay on-site (privacy, compliance, or multi-plant). Each site runs a Flower client that trains on local TimescaleDB data; only model weight updates are sent to a central server. Data never leaves each client process.
- **Centralized training (default):** Use when all data can be aggregated or for single-tenant deployments. The existing `train_model.py` pipeline (FLAML/XGBoost) runs as today; retraining is triggered by drift CRITICAL or label milestones.

Set `FL_ENABLED=true` and `FL_SERVER_ADDRESS=<host:port>` so that Prefect flows (drift CRITICAL, scheduled retraining) invoke the Flower client instead of the centralized `train_model.py` subprocess.

### How to start the FL server

With Docker (profile `federated`):

```bash
docker-compose --profile federated up fl-server
```

Or locally:

```bash
python -m federated.fl_server --num-rounds 10 --port 8080 --min-clients 2
```

The server runs FedAvg (Federated Averaging), requires at least 2 clients before aggregating, and logs the aggregated model to MLflow after each round (if MLflow is configured).

### How to register a new client site

At each site (or machine), run the Flower client with that siteâ€™s `machine_id` and the central server address. Data is loaded from the local TimescaleDB (labeled feature snapshots for that machine); the same canonical feature schema is used across all clients.

```bash
python -m federated.fl_client --server-address central-server:8080 --machine-id WB-001
```

Ensure the site has:

- TimescaleDB (or DB connection) with labeled data for that `machine_id` (from the labeling UI or API).
- The same canonical feature set (see `federated/constants.py`).

Optional: enable differential privacy with `FL_DP_ENABLED=true`, `FL_DP_CLIPPING_NORM`, and `FL_DP_NOISE_MULTIPLIER` (see `federated/privacy_config.py`).

---

## ğŸš€ Quick Start

### One-command development setup

From the repository root, run:

```bash
./scripts/setup_dev.sh
```

This script:

- Checks for **Python 3.11+**, **Docker**, **Docker Compose**, and **Node 18+** (prints install hints if missing)
- Copies `.env.example` â†’ `.env` when `.env` does not exist
- Creates a venv, installs `requirements.txt`, starts **TimescaleDB** and **Redis** with Docker Compose, waits for the DB to be ready, runs **`alembic upgrade head`**, and installs frontend deps (`cd frontend && npm install`)
- Prints a success summary with URLs: **API** http://localhost:8000, **Frontend** http://localhost:5173

### Verify connections

After setup, confirm infrastructure and optional hardware:

```bash
source .venv/bin/activate   # if not already active
python scripts/verify_connections.py
```

This checks **TimescaleDB**, **Redis**, **ABB RWS** (`ROBOT_IP:ROBOT_PORT/rw/system`), and **Siemens S7** (snap7 to `SIEMENS_PLC_IP`) and prints a pass/fail table. Configure `ROBOT_IP`, `ROBOT_PORT`, and `SIEMENS_PLC_*` in `.env` for virtual or physical hardware (see `.env.example`).

### Run the stack

```bash
# Activate venv
source .venv/bin/activate

# Start API
uvicorn api_server:app --host 0.0.0.0 --port 8000 --reload

# In another terminal: start frontend (Vite dev server on 5173)
cd frontend && npm run dev
```

**If the app shows "cannot connect" or the API is unreachable:**

1. **Start the API** â€” In a terminal from the project root: `uvicorn api_server:app --host 0.0.0.0 --port 8000 --reload`. The frontend (Vite on 5173) proxies `/api` and `/ws` to port 8000.
2. **Start PostgreSQL and Redis** â€” Either run `docker compose up -d` (or the services from `./scripts/setup_dev.sh`) so the API can connect to the database and Redis.
3. **Check .env** â€” Ensure `.env` exists (copy from `.env.example`) and has `DB_PASSWORD` and `SECURITY_JWT_SECRET` set; otherwise the API may fail on startup.

Optional: start mock streaming with `python stream_publisher.py` and `python stream_consumer.py`.

### Full NASA dataset (training)

To use the full NASA datasets for training:

```bash
python scripts/download_nasa_data.py
# Then use "Initialize NASA Data" in the Pipeline Operations dashboard if available
```

See [NASA C-MAPSS](https://data.nasa.gov/dataset/cmapss-jet-engine-simulated-data) and [NASA IMS Bearings](https://data.nasa.gov/dataset/ims-bearings) for dataset details.

### Stream historical data (free datasets)

To test the pipeline with **realistic historical data** instead of synthetic:

1. **Azure Predictive Maintenance** (best match): Download the [Microsoft Azure PdM dataset](https://www.kaggle.com/datasets/arnabbiswas1/microsoft-azure-predictive-maintenance/data) (free with Kaggle account), place the 5 `PdM_*.csv` files in `data/azure_pm/`, then run:
   ```bash
   python scripts/prepare_historical_stream.py
   python mock_fleet_streamer.py --source azure_pm --speed-multiplier 720
   ```
   This loads telemetry into the DB and replays it to Redis with time compression; the dashboard will show machines and health as the replay runs.

2. **NASA / FEMTO / CWRU**: See [docs/FREE_DATASETS_STREAMING.md](docs/FREE_DATASETS_STREAMING.md) for links and steps for each dataset.

---

## ğŸ“¡ API Endpoints

### Public Endpoints
| Method | Path | Description |
|--------|------|-------------|
| GET | `/health` | Load balancer health check |
| GET | `/docs` | Swagger UI documentation |
| GET | `/api/machines` | List all machines with status |
| GET | `/api/features` | Historical feature data |
| GET | `/api/recommendations/{machine_id}` | AI maintenance recommendations |
| WS | `/ws/stream?token=...` | Real-time telemetry stream |

### Authenticated Endpoints
| Method | Path | Description |
|--------|------|-------------|
| POST | `/api/enterprise/token` | Get JWT token (login) |
| GET | `/api/enterprise/alarms` | List alarms |
| POST | `/api/enterprise/alarms` | Create alarm |
| GET | `/api/enterprise/work-orders` | List work orders |
| POST | `/api/enterprise/work-orders` | Create work order |
| GET | `/api/enterprise/reliability/{machine_id}` | MTBF/MTTR metrics |
| GET | `/api/enterprise/schedule` | Shift schedule |
| POST | `/api/analytics/trigger` | Trigger background analytics |

### WebSocket Protocol
```javascript
// Connect with JWT token
const ws = new WebSocket('ws://localhost:8000/ws/stream?token=YOUR_JWT');

// Message types received:
{ "type": "telemetry", "machine_id": "WB-001", "failure_probability": 0.25, ... }
{ "type": "heartbeat", "timestamp": "2024-01-13T10:00:00Z" }
{ "type": "error", "error": "Processing error" }
```

---

## ğŸ—„ï¸ Database Schema (Key Tables)

```sql
-- Time-series sensor features
cwru_features (
    id, timestamp, machine_id,
    peak_freq_1..5, peak_amp_1..5,
    bpfo_amp, bpfi_amp, bsf_amp, ftf_amp,
    degradation_score, failure_prediction, failure_class
)

-- Alarm management
pdm_alarms (
    alarm_id, machine_id, severity, code, message,
    trigger_type, trigger_value, threshold_value,
    active, acknowledged, acknowledged_by, resolved_at
)

-- Work order tracking
work_orders (
    work_order_id, machine_id, title, description,
    priority, status, work_type,
    scheduled_date, estimated_duration_hours,
    assigned_to, actual_duration_hours, notes
)

-- Failure event history
failure_events (
    id, machine_id, timestamp, event_type,
    failure_probability, degradation_score
)
```

---

## ğŸ¤– ML Models

### 1. Failure Prediction Model (`gaia_model.pkl`)
- **Type**: XGBoost Classifier
- **Features**: 28 spectral + bearing fault features
- **Output**: `failure_probability` (0-1), `failure_class`
- **Training**: `train_rul_model.py`

### 2. RUL Model (`rul_model.json`)
- **Type**: Gradient Boosting Regressor
- **Output**: Remaining Useful Life in days
- **Formula**: `RUL = 2000 * (1 - degradation)Â² / 24`

### 3. Anomaly Detection (`anomaly_discovery/`)
- **Isolation Forest**: Point anomaly detection
- **Temporal Autoencoder**: Sequence anomaly detection
- **Ensemble**: Combined scoring

---

## âš™ï¸ Configuration

All configuration is centralized in `config.py` using Pydantic Settings:

```python
# Key settings loaded from .env:
settings.database.url          # DATABASE_URL
settings.redis.host            # REDIS_HOST
settings.security.secret_key   # SECRET_KEY
settings.security.algorithm    # HS256
settings.security.token_expire # 30 minutes
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test file
pytest tests/test_schema_validation.py -v
```

---

## ğŸ“Š Equipment Metadata

The system tracks these machine types:

| ID | Name | Shop | Line | Type |
|----|------|------|------|------|
| WB-001 | 6-Axis Welder #1 | Body Shop | Underbody Weld Cell | Spot Welder |
| WB-002 | 6-Axis Welder #2 | Body Shop | Underbody Weld Cell | Spot Welder |
| HP-200 | Hydraulic Press 2000T | Stamping | Press Line 1 | Hydraulic Press |
| PR-101 | Paint Robot #1 | Paint Shop | Sealer Line | Paint Applicator |
| TS-001 | Torque Station #1 | Final Assembly | Chassis Line | Torque Tool |
| CV-100 | Main Conveyor Drive | Final Assembly | Chassis Line | Conveyor Motor |

---

## ğŸ“ Development Conventions

### Code Style
- **Python**: Black + Ruff (linting)
- **JavaScript**: ESLint + Prettier
- **Commits**: Conventional Commits (`feat:`, `fix:`, `docs:`)

### File Naming
- Python: `snake_case.py`
- React Components: `PascalCase.jsx`
- Schemas: Pydantic models in `schemas/`
- Services: Business logic in `services/`

### Error Handling
```python
# Use custom exceptions from core/exceptions.py
from core.exceptions import ResourceNotFound, BusinessRuleViolation

# Returns clean JSON, no stack traces
raise ResourceNotFound("machine", machine_id)
```

---

## ğŸ”„ Data Flow

```
1. Sensors â†’ OPC-UA/MQTT â†’ stream_publisher.py
2. stream_publisher.py â†’ Redis (sensor_stream channel)
3. stream_consumer.py â† Redis â†’ PostgreSQL (cwru_features)
4. api_server.py â† PostgreSQL â†’ ML Inference
5. WebSocket â†’ Frontend (real-time updates)
```

---

## ğŸ“š Related Documentation

- [SECURITY.md](./SECURITY.md) - Security policies and practices
- [CONTRIBUTING.md](./CONTRIBUTING.md) - Contribution guidelines
- [docs/AZURE_OPENAI_MIGRATION.md](./docs/AZURE_OPENAI_MIGRATION.md) - AI service migration
- [OPCUA_README.md](./OPCUA_README.md) - OPC-UA integration guide

---

## ğŸ“„ Repository

**PDM-PILOT** â€” Push to: `https://github.com/andrewgt3/PDM-PILOT`

```bash
git remote set-url origin https://github.com/andrewgt3/PDM-PILOT.git
git push origin main
```

---

## ğŸ“„ License

Proprietary - PlantAGI / AIJ Engineering Consulting

---

## ğŸ‘¥ Team

**PlantAGI** - Industrial AI Solutions  
Developed by AIJ Engineering Consulting
